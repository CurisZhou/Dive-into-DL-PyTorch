
''''''

from collections import Counter
import torch
from torch import nn
from torch.utils import data
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torch import optim


class Vocab(object):
    def __init__(self, all_text_samples_tokens, min_freq=0, use_special_tokens=True, lowercase=False):
        ''''''
        '''
        # 参数解释
        # (1) all_text_samples_tokens： 为所有的文本样本(all text samples)进行tokenization之后的所有tokens不去重之后存入的一个列表,
        # 即一个存放所有文本样本不去重tokens的列表

        # (2) min_freq： min_freq为指定该保留的token的最低频数的参数,所有tokens在输入Counter()对象进行频数统计后,根据min_freq参数舍弃频数低于min_freq的token

        # (3)use_special_tokens：use_special_tokens为布尔型,指定是否在根据tokens创建的"token索引列表idx_to_token"和"语料库索引词典token_to_idx"中添加
        # 特殊符号如：填充特殊符<pad>、序列文本开头符<bos>、序列文本结束符<eos>。
        # 而未登录词特殊符<unk>不管use_special_tokens为True还是False,其都要被添加进"token索引列表idx_to_token"和"语料库索引词典token_to_idx"中

        # (4)lowercase: 若tokens为英文单词,是否将所有英文tokens全转为小写;中文tokens默认为False
        '''
        if lowercase: all_text_samples_tokens = [token.lower() for token in all_text_samples_tokens]

        tokens_counter = Counter(all_text_samples_tokens)

        # 对tokens_counter先按照其tokens进行排序,再按照每个token的频数进行排序
        # 此时的token_freqs为一个元组列表,列表中每一个元组包含两个元素,第一个元素为token,第二个元素为其对应的频数
        token_freqs = sorted(tokens_counter.items(), key=lambda x: x[0])
        token_freqs.sort(key=lambda x: x[1], reverse=True)  # 默认为升序排列,此处设置为降序排列

        # idx_to_token为一个包含了所有不重复tokens的列表
        # idx_to_token列表中字符的位置索引即为 "语料库索引词典token_to_idx" 中字符对应的索引号,
        # 因此可依据一个字符在"语料库索引词典token_to_idx" 中的索引号直接在idx_to_token列表的相应位置索引处获取对应的字符
        self.idx_to_token = []
        if use_special_tokens:
            # 先给填充特殊符<pad>、序列文本开头符<bos>、序列文本结束符<eos>、未登录词特殊符<unk>,这四种特殊符号各赋予一个不重复的索引
            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)
            self.idx_to_token += ["<pad>", "<bos>", "<eos>", "<unk>"]
        else:
            self.idx_to_token += ["<unk>"]

        # 将token_freqs中所有tokens(已去重),按照对应频数大于min_freq的要求添加入"token索引列表idx_to_token"中
        self.idx_to_token.extend([token for token, freq in token_freqs if freq >= min_freq])

        # 根据idx_to_token列表,构建"语料库索引词典token_to_idx",即idx_to_token列表中不重复的token与其对应的索引号组成的字典
        # 在此处由于token_freqs中统计频数越多的token在idx_to_token列表中的索引位置越靠前,因此统计频数越多的token在"语料库索引词典token_to_idx"中
        # 的对应的索引号也就越小
        self.token_to_idx = dict([(token, idx) for idx, token in enumerate(self.idx_to_token)])

        # 语料库词表的大小
        self.vocab_size = len(self.idx_to_token)

    # 魔法函数 __len__可以让实例化的类Vocab在在调研len()函数后返回词表大小
    def __len__(self):
        return self.vocab_size


    # 魔法函数 __getitem__ 可以让实例化的类Vocab像列表一样索引并返回值
    # 此处为将一个利用列表索引方式输入的tokens列表转换为相应的idx列表
    def __getitem__(self, tokens):
        # 若此时的tokens实际为单个token,而不是一个列表或者元组
        if not isinstance(tokens, (list, tuple)):
            # 字典的dict().get(D,d)可接受两个参数,若字典中有D这个键(key),则返回D主键对应的值(value);若无D主键,则返回d这个值本身
            return self.token_to_idx.get(tokens, self.unk)

        return [self.__getitem__(token) for token in tokens]


    # 此时将一个输入的tokens索引列表indices由索引idx形式, 转换为其原本的token形式
    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]

        return [self.to_tokens(index) for index in indices]




def src_trg_corpus_creation():
    with open("fraeng6506.txt","r") as f:
        # 此时从英法对照翻译文件中读取的raw_text为一整个长字符串
        raw_text = f.read()
    f.close()

    '''
    字符在计算机里是以编码的形式存在，我们通常所用的空格是 \x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。 
    而 \xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。
    '''
    def preprocess_raw_text(raw_text):
        # 将原始文本的长字符串中的超出gbk编码范围的拉丁空格符替换为gbk编码中的标准空格符
        raw_text = raw_text.replace("\u202f"," ").replace("\xa0"," ")
        preprocess_text = ""

        # 循环原始文本的长字符串中的每个字符,如果循环到的字符是特殊标点符号,且其前一个字符不是空格符,则应在将这个字符添加
        # 到preprocess_text中之前先添加一个空格符,即保证原始文本的长字符串中所有的特殊标点符号之前都有一个空格符号
        for i,char in enumerate(raw_text):
            if char not in (",",".","!") and i > 0 and raw_text[i-1] != " ":
                preprocess_text += " "
            preprocess_text += char

        return preprocess_text

    preprocess_text = preprocess_raw_text(raw_text)

    '''
    对原始英法翻译文本的长字符串按换行符\n进行分割； 再将分割出的每行文本line按照制表符\t进行分割；每行line按照制表符\t分割
    后的元素列表中的第一个元素为英文句子,将每个英文句子按照空格符分割后的英文tokens列表存入src_corpus列表中；每行line按照制表符\t分割
    后的元素列表中的第二个元素为法文句子,将每个法文句子按照空格符分割后的法文tokens列表存入trg_corpus列表中
    '''
    # src_corpus为待翻译语料库(英文),trg_corpus翻译后语料库(法文)；
    # src_corpus与trg_corpus都为列表中套着多个列表的结构：src_corpus列表中每个列表都存储着一个英文句子分词后的所有tokens;
    # trg_corpus列表中每个列表都存储着一个法文句子分词后的所有tokens
    src_corpus,trg_corpus = [],[]

    # 对原始英法翻译文本的长字符串按换行符\n进行分割； 再将分割出的每行文本line按照制表符\t进行分割
    for i,line in enumerate(preprocess_text.split("\n")):
        parts = line.split("\t")

        # 保证每一行line都有一句英文句子与一句法文句子
        if len(parts) > 2:
            # 每行line按照制表符\t分割后的元素列表中的第一个元素为英文句子,将每个英文句子按照空格符分割后的英文tokens列表存入source列表中；
            # 每行line按照制表符\t分割后的元素列表中的第二个元素为法文句子,将每个法文句子按照空格符分割后的法文tokens列表存入target列表中
            src_corpus.append(parts[0].split(" "))
            trg_corpus.append(parts[1].split(" "))

    return src_corpus, trg_corpus


def src_trg_vocab_creation(src_corpus,trg_corpus):
    ''''''
    ''' 
    src_corpus为待翻译语料库(英文),trg_corpus翻译后语料库(法文)；
    src_corpus(即source)与trg_corpus(即target)都为列表中套着多个列表的结构：src_corpus列表中每个列表都存储着一个英文句子分词后的所有tokens;
    trg_corpus列表中每个列表都存储着一个法文句子分词后的所有tokens
    '''

    # 根据所有英文句子分词后所有的tokens创建对应的Vocab类,此Vocab类中有待翻译的英文语料库的"idx_to_token列表"以及"语料库索引词典token_to_idx"
    src_all_tokens = [token for line in src_corpus for token in line]
    src_vocab = Vocab(all_text_samples_tokens=src_all_tokens,min_freq=0,use_special_tokens=True)

    # 根据所有法文句子分词后所有的tokens创建对应的Vocab类,此Vocab类中有翻译后的法文语料库的"idx_to_token列表"以及"语料库索引词典token_to_idx"
    trg_all_tokens = [token for line in trg_corpus for token in line]
    trg_vocab = Vocab(all_text_samples_tokens=trg_all_tokens, min_freq=0, use_special_tokens=True)

    return src_vocab, trg_vocab




# pad函数用来将语料库中每个句子样本数据(line)的所有tokens进行索引(idx)转换后的序列利用"特殊填充符号索引padding_token_idx"填充
# 为最大长度max_len； 若经过索引(idx)转换后的序列长度超过max_len，则将此序列截取前max_len长度的序列保留,剩余的序列舍弃.
def pad(line, max_len, padding_token_idx):
    if len(line) > max_len:
        return line[0:max_len]

    return line + [padding_token_idx] * (max_len - len(line))


# 此函数用来将所有待翻译语料库(英文)src_corpus,或者所有翻译后语料库(法文)trg_corpus转换为张量数据形式以方便输入模型中进行模型训练
def build_tensor(corpus, vocab, max_len, is_source=True):
    ''''''
    '''
    (1)corpus可能为src_corpus或者trg_corpus;
    
    (2)vocab可能为src_vocab或者trg_vocab;
       
    (3)max_len为语料库中每个句子样本数据(line)的所有tokens进行索引(idx)转换后的序列利用"特殊填充符号索引
        padding_token_idx"填充的最大长度
       
    (4)is_source参数用来判断输入的语料库为待翻译语料库(英文)src_corpus,还是所有翻译后语料库(法文)trg_corpus；
        如果输入的为翻译后语料库(法文)trg_corpus,则要对trg_corpus语料库中每个句子样本数据的所有tokens进行索引(idx)转换后
        的序列的开头与结尾分别加上序列文本开头符<bos>与序列文本结束符<eos>所分别对应的索引1与2
    '''

    if is_source:
        # 对src_corpus语料库中每个句子样本数据的所有tokens进行索引(idx)转换
        corpus_idx = [vocab[line] for line in corpus]
    else:
        # 对trg_corpus语料库中每个句子样本数据的所有tokens进行索引(idx)转换后的序列的开头与结尾分别加上序列文本开头符<bos>与
        # 序列文本结束符<eos>所分别对应的索引1与2
        corpus_idx = [[vocab.bos] + vocab[line] + [vocab.eos] for line in corpus]

    # 此时的corpus_idx为一个包含了语料库中每个句子样本数据的所有tokens进行索引(idx)转换后的序列列表的列表(即为列表中套许多列表的结构)
    # 因此其可直接用torch中的tensor()方法直接转换为张量；但语料库中每个句子样本数据(line)的所有tokens进行索引(idx)转换后的序列
    # 必须要利用"特殊填充符号索引padding_token_idx"填充为最大长度max_len
    corpus_tensor = torch.tensor( [ pad(line=line,max_len=max_len,padding_token_idx=vocab.pad) for line in corpus_idx ] )

    # 此时corpus_tensor张量中每一行即代表语料库中每一个句子样本数据,而每一行的所有列的数字即代表这一个句子样本数据的所有tokens
    # 进行索引(idx)转换后的索引数；由于每一个句子样本数据所包含的tokens数量不同,因此在之前已经用Vocab类中的填充特殊符<pad>的索引值
    # 进行了特殊填充,因此此时corpus_tensor张量中每一行的所有列的数字中有些数字是填充特殊符<pad>的索引值,而这些值并不能算作
    # 语料库中一个句子样本数据的有效tokens长度,除去填充特殊符<pad>的索引值的列数才是一个句子样本数据的有效tokens长度，因此要计算
    # 所有句子样本数据的有效tokens长度,便可用(corpus_tensor != vocab.pad)先进行判断,再用sum(1)进行列方向的横向相加,便计算出了
    # 所有句子样本数据的有效tokens长度. 此时valid_len张量为一个一维张量, 其形状维度为: (batch_size,)
    valid_len = (corpus_tensor != vocab.pad).sum(1)   # valid_len张量

    return corpus_tensor, valid_len


def load_two_corpuses_tensor_data(max_len=10, batch_size=32):
    # 创建待翻译语料库(英文)src_corpus,与翻译后语料库(法文)trg_corpus
    src_corpus,trg_corpus = src_trg_corpus_creation()
    # 创建待翻译语料库(英文)src_corpus的语料库词典Vocab类,与翻译后语料库(法文)trg_corpus的语料库词典Vocab类
    src_vocab, trg_vocab = src_trg_vocab_creation(src_corpus,trg_corpus)

    # 将所有待翻译语料库(英文)src_corpus,以及所有翻译后语料库(法文)trg_corpus转换为张量数据形式,并获取对应张量在每一行上的有效长度valid_len张量
    src_tensor, src_valid_len = build_tensor(corpus=src_corpus, vocab=src_vocab, max_len=max_len, is_source=True)
    trg_tensor, trg_valid_len = build_tensor(corpus=trg_corpus, vocab=trg_vocab, max_len=max_len, is_source=False)

    # 将src_tensor, src_valid_len, trg_tensor, trg_valid_len四个张量可以一起放入torch.utils.data.TensorDataset()类中组合起来,
    # 以方便后续放入torch.utils.data.DataLoader()类中进行批量batch数据生成以方便迭代；此外torch.utils.data.TensorDataset()类
    # 还可以检查输入其中的所有张量的第一个维度是否一致(即行数是否一致)
    train_data = torch.utils.data.TensorDataset(src_tensor, src_valid_len, trg_tensor, trg_valid_len)
    train_iter = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)

    return src_vocab, trg_vocab, train_iter


# 编码器类Seq2SeqEncoder()在此处用来将需要进行翻译的英文语料库的batch样本数据按照seq_len、且利用深度循环神经网络: GRU(num_layers为2)
# 进行语义编码, 最后深度GRU对输入的待翻译英文batch样本数据的语义编码即为: 深度GRU网络最后一个时间步下的hiddeb state，即h_t。而
# 对于编码器输出的两个结果enc_gru_outputs与enc_gru_final_state, 此处也仅需用到待翻译英文batch样本数据的语义编码enc_gru_final_state,
# 而语义编码enc_gru_final_state可以作为解码器Seq2SeqDecoder()的初始隐藏状态h_0(解码器深度GRU的h_0),输入进解码器Seq2SeqDecoder()中
class Seq2SeqEncoder(nn.Module):
    def __init__(self, src_vocab_size:int, embed_size:int, enc_hid:int, num_layers=2, dropout=0):
        super(Seq2SeqEncoder, self).__init__()
        self.enc_hid = enc_hid
        # 在此构建深度循环神经网络GRU对待翻译英文batch样本数据进行语义编码, 此处默认设置深度循环神经网络GRU的隐藏层数为2(即num_layers参数为2)
        self.num_layers = num_layers

        # 词嵌入Embedding层
        self.embedding = torch.nn.Embedding(src_vocab_size, embed_size)
        self.enc_gru = torch.nn.GRU(input_size=embed_size, hidden_size=enc_hid, num_layers=self.num_layers ,dropout=dropout)

    def init_hidden_0(self,batch_size):
        return torch.zeros( (self.num_layers, batch_size, self.enc_hid) )

    def forward(self,input:torch.Tensor):
        # input表示待翻译的英文batch样本数据张量
        # 此时待翻译的英文batch样本数据的输入形状一开始为: (batch_size, seq_len)，由于在预处理部分对待翻译的英文batch样本中每条
        # tokens索引化后的样本数据进行了空白特殊符<pad>的索引填充(空白特殊符<pad>索引为0),使得所有的待翻译英文batch样本中每条
        # tokens索引化后的样本数据的长度都一致,为设定的超参数max_len的值，因此此处输入的待翻译英文batch样本数据形状中的seq_len即
        # 相当于是max_len.
        batch_size = input.shape[0]

        # 此时经过词嵌入之后,待翻译英文batch样本数据即变为一个三维张量,形状为: (batch_size, seq_len, embed_size)
        input = self.embedding(input)

        # 此处利用张量的permute()函数将input张量的第一维与第二维交换,此时input张量的形状为(seq_len, batch_size，embed_size),
        # 这样input张量才符合输入GRU网络层的要求
        input = input.permute(1, 0, 2)

        # 获取输入GRU循环神经网络中的,第0个时间步(time_step_0)的隐藏状态h_0张量,h_0的张量形状为(num_layers * num_directions, batch_size, enc_hid)
        h_0 = self.init_hidden_0(batch_size)

        # 此时enc_gru_outputs张量的形状为: (seq_len, batch_size, enc_hid)。
        # 此时enc_gru_final_state张量的形状为: (num_layers * num_directions, batch_size, enc_hid) --> (2, batch_size, enc_hid),
        # 为最后一个时间步两个隐藏层的隐藏状态。
        enc_gru_outputs, enc_gru_final_state = self.enc_gru(input, h_0)
        # 此时enc_gru_outputs张量的形状为: (batch_size, seq_len, enc_hid)
        # enc_gru_outputs = enc_gru_outputs.permute(1, 0, 2)

        # 对于编码器输出的两个结果enc_gru_outputs与enc_gru_final_state, 此处也仅需用到待翻译英文batch样本数据的语义编码enc_gru_final_state,
        # 而语义编码enc_gru_final_state可以作为解码器Seq2SeqDecoder()的初始隐藏状态h_0(解码器深度GRU的h_0),输入进解码器Seq2SeqDecoder()中
        return enc_gru_outputs, enc_gru_final_state


# Seq2SeqAttention()为注意力机制的类，其在此处,利用Seq2SeqEncoder编码器对待翻译的英文batch样本数据张量进行计算后每一时间步(time step)的
# 隐藏状态hidden_state(enc_gru_outputs张量)和编码器最后一个时间步两个隐藏层的隐藏状态的语义编码张量enc_gru_final_state, 来计算得出一个
# 编码器每一时间步(time step)的隐藏状态hidden_state的加权求和的结果张量, 这个加权求和的结果张量的形状为: (1, batch_size, enc_hid)
''' 此时enc_gru_outputs张量的形状为: (seq_len, batch_size, enc_hid)。
此时enc_gru_final_state张量的形状为: (num_layers * num_directions, batch_size, enc_hid) --> (2, batch_size, enc_hid)'''
class Seq2SeqAttention(nn.Module):
    def __init__(self,enc_hid: int,
                 dec_hid: int,
                 atten_dim = 8):
        self.enc_hid = enc_hid
        self.dec_hid = dec_hid
        self.atten_dim = atten_dim

        # 此全连接层用于将编码器最后一个时间步两个隐藏层的隐藏状态的语义编码张量enc_gru_final_state中的最后一个隐藏层隐藏状态的
        # 语义编码张量的形状由(batch_size, enc_hid)转换为(batch_size, dec_hid), 以便用于之后注意力机制加权求和结果张量的计算
        self.fc_enc_gru_final_state = nn.Linear(self.enc_hid, self.dec_hid)

        #此全连接层用于将在第三个维度(dim=2)上拼接起来的enc_gru_outputs张量(此时形状应变为(batch_size, seq_len, enc_hid))和
        # 变换后的最后一个隐藏层隐藏状态的语义编码张量(此时形状为(batch_size, seq_len, dec_hid)),的第三个维度(dim=2)由enc_hid + dec_hid变换
        # 为atten_dim
        self.fc_atten_transform = nn.Linear(self.enc_hid + self.dec_hid, self.atten_dim)


    def forward(self,enc_gru_outputs: torch.Tensor,
                 enc_gru_final_state: torch.Tensor):
        # 此时enc_gru_outputs张量的形状由(seq_len, batch_size, enc_hid) 变为 (batch_size, seq_len, enc_hid)
        enc_gru_outputs = enc_gru_outputs.permute(1,0,2)
        seq_len =  enc_gru_outputs.shape[1]

        # 此时enc_gru_final_state张量的形状由(num_layers * num_directions, batch_size, enc_hid) --> (2, batch_size, enc_hid)变为(batch_size, enc_hid)
        enc_gru_final_state = enc_gru_final_state[-1]
        # 此时enc_gru_final_state张量的形状变为(batch_size, dec_hid)
        enc_gru_final_state = F.tanh( self.fc_enc_gru_final_state(enc_gru_final_state) )
        # 此时enc_gru_final_state张量的形状变为(batch_size, seq_len, dec_hid)
        enc_gru_final_state = enc_gru_final_state.unsqueeze(1).repeat(1,seq_len,1) # repeat()表示将张量第二个维度扩展为原先的seq_len倍

        # 此时atten_energy张量形状为(batch_size, seq_len, enc_hid + dec_hid)
        atten_energy = torch.cat((enc_gru_outputs, enc_gru_final_state), dim=2)
        # 此时atten_energy张量形状为(batch_size, seq_len, atten_dim)
        atten_energy = F.tanh( self.fc_atten_transform(atten_energy) )

        # 此时attention张量即为编码器每一时间步(time step)的隐藏状态hidden_stat的加权求和时所需的权重值张量,其形状为(batch_size, seq_len)
        attention = F.softmax( atten_energy.sum(2), 1)
        # 此时attention张量形状变为(batch_size, 1, seq_len)
        attention = attention.unsqueeze(1)

        # torch.bmm()方法是将两个张量按照第1个维度batch_size的方向进行全连接内积转换(此时输入的两个张量的第1维必须相同且为batch_size,
        # 且第一个张量的第3个维度必须等于第二个张量的第2个维度,这样才能让两个张量按照第1个维度batch_size的方向进行全连接内积转换)
        # 此时attention张量形状为(batch_size, 1, seq_len)，enc_gru_outputs张量形状为(batch_size, seq_len, enc_hid)
        # 最后得出的编码器每一时间步(time step)的隐藏状态hidden_stat的加权求和的结果张量weighted_enc_gru_outputs形状为: (batch_size, 1, enc_hid)
        weighted_enc_gru_outputs = torch.bmm(attention, enc_gru_outputs).permute(1,0,2)
        # 注意,这里要使用permute()方法使weighted_enc_gru_outputs张量形状变为(1, batch_size, enc_hid),这样才方便在之后的
        # 解码器Seq2SeqDecoder()中与每一个时间步(time step)输入的词嵌入后的翻译后法语batch样本数据在第三个维度(dim=2)上进行拼接,再输入解码器中进行预测
        weighted_enc_gru_outputs = weighted_enc_gru_outputs.permute(1,0,2)

        return weighted_enc_gru_outputs


# (1) Seq2SeqDecoder()类在训练模式下,先获得编码器类Seq2SeqEncoder()计算的最后一个时间步两个隐藏层的隐藏状态的语义编码张量enc_gru_final_state
# 作为解码器类中深度GRU循环网络的两个隐藏层的初始隐藏状态h_0; 之后,将翻译后法语batch样本数据输入解码器中的深度GRU循环网络中进行模型训练；此时,
# 将解码器的深度GRU循环网络中的每一个时间步(time step)后隐藏层输出的隐藏状态(张量形状为(batch_size, dec_hid))再输入到一个输出全连接层中,输出全连接层
# 将每一个时间步(time step)后隐藏层输出的隐藏状态变为输出状态,形状也变为(batch_size, trg_vocab_size),这样深度GRU循环网络中的每一个时间步都会预测
# 下一个翻译后法文token的索引值是什么(即target token),每一个时间步的"预测下一个法文token索引值"会在计算总损失阶段与每一个
# 时间步后"真实下一个法文token索引值"进行对比以计算每个时间步后预测值的loss值。

# (2)根据上方内容,可以看出解码器Seq2SeqDecoder()类在训练阶段应使用自监督模式进行训练(一种teacher模式)；自监督模式即为用一个翻译后
# 法语数据样本的seq_len中从0到seq_len - 1(即[0:-1])的索引位置上切片下的数据作为解码器训练数据(相当于trg_batch[:, 0:-1]),
# 而从1到seq_len (即[1:])的索引位置上切片下的数据作为解码器训练的真实标签(相当于trg_batch[:, 1:]); 自监督模式使得切片0到seq_len - 1得到
# 的编码器训练数据相当于少了一个翻译后法文样本数据seq_len最末尾的"序列文本结束符<eos>"所对应的词表索引2, 而使得切片1到seq_len得到的
# 解码器训练的真实标签相当于少了一个翻译后法文样本数据seq_len开头处的"序列文本开头符<bos>"所对应的词表索引1.

# (3)当解码器Seq2SeqDecoder()在测试模式,即翻译并预测一句英文句子所对应的法文句子的模式下,不使用自监督模式进行训练,此时没有训练过程。
# 此时仅向解码器Seq2SeqDecoder()中输入一个batch_size为1,seq_len也为1的"序列文本开头符<bos>"所对应的词表索引为1,所构成的翻译后法文句子batch样本数据，
# 即tensor( [[1]] ), 形状为(1, 1)。之后,解码器深度GRU循环网络的第一个时间步(time step)基于"序列文本开头符<bos>"所对应的词表索引1预测的
# 下一个翻译后法文token索引将作为深度GRU循环网络的下一个时间步的输入来预测第二个翻译后法文token索引,以此类推,直到某一个时间步的
# 预测翻译后法文token索引为"序列文本结束符<eos>"所对应的词表索引2时, 深度GRU循环网络结束,解码器翻译预测完成。
class Seq2SeqDecoder(nn.Module):
    def __init__(self,trg_vocab_size:int, embed_size:int, dec_hid:int, enc_hid:int, num_layers=2, dropout=0):
        self.dec_hid = dec_hid
        self.enc_hid = enc_hid
        # 此处默认设置解码器的深度GRU循环网络的隐藏层数为2(即num_layers参数为2)
        self.num_layers = num_layers

        # 由于在解码器中会输入编码器每一时间步(time step)的隐藏状态hidden_stat的加权求和的结果张量weighted_enc_gru_outputs(形状为(1, batch_size, enc_hid)),
        # 因此无论在训练阶段还是测试预测阶段,其都会与输入解码器后经过词嵌入的batch样本数据(形状为(seq_len, batch_size, embed_size))
        # 在第三个维度(dim=2)进行拼接，拼接后的张量才作为训练batch样本数据或者测试(预测)batch样本数据输入到解码器的深度GRU循环网络中,
        # 其形状为(seq_len, batch_size, embed_size + enc_hid)
        self.embedding = nn.Embedding(trg_vocab_size, embed_size)
        self.dec_gru = nn.GRU(embed_size + enc_hid, dec_hid, num_layers=self.num_layers, dropout=dropout)

        # 将解码器的深度GRU循环网络中的每一个时间步(time step)后隐藏层输出的隐藏状态(张量形状为(batch_size, dec_hid))再输入到一个输出全连接层中,输出全连接层
        # 将每一个时间步(time step)后隐藏层输出的隐藏状态变为输出状态,形状也变为(batch_size, trg_vocab_size),这样深度GRU循环网络中的每一个时间步都会预测
        # 下一个翻译后法文token的索引值是什么(即target token),每一个时间步的"预测下一个法文token索引值"会在计算总损失阶段与每一个
        # 时间步后"真实下一个法文token索引值"进行对比以计算每个时间步后预测值的loss值。
        self.fc_decoder_to_predictions = nn.Linear(dec_hid, trg_vocab_size)

    def init_hidden_0(self,batch_size):
        return torch.zeros( (self.num_layers, batch_size, self.dec_hid) )

    def forward(self,input:torch.Tensor,
                enc_gru_final_state:torch.Tensor,
                weighted_enc_gru_outputs:torch.Tensor):
        # input表示翻译后法文batch样本数据张量。
        # 此时翻译后法文batch样本数据张量的输入形状一开始为: (batch_size, seq_len)。实际上,此时的seq_len为最初翻译后法文batch样本数据
        # 的seq_len减去1,因为在训练阶段应使用自监督模式进行训练(一种teacher模式)；自监督模式即为用一个翻译后法语数据样本的seq_len中
        # 从0到seq_len - 1(即[0:-1])的索引位置上切片下的数据作为解码器训练数据(相当于trg_batch[:, 0:-1]),而从1到seq_len (即[1:])的
        # 索引位置上切片下的数据作为解码器训练的真实标签(相当于trg_batch[:, 1:])
        batch_size = input.shape[0]

        # 此时经过词嵌入之后,翻译后法文batch样本数据即变为一个三维张量,形状为: (batch_size, seq_len, embed_size)
        input = self.embedding(input)
        # 此时input张量的形状为(seq_len, batch_size，embed_size),这样input张量才符合输入GRU网络层的要求
        input = input.permute(1, 0, 2)

        # 获得编码器类Seq2SeqEncoder()计算的最后一个时间步两个隐藏层的隐藏状态的语义编码张量enc_gru_final_state作为解码器类中
        # 深度GRU循环网络的两个隐藏层的初始隐藏状态h_0, h_0的张量形状为(num_layers * num_directions, batch_size, enc_hid) 即 (2, batch_size, enc_hid).
        # 若此时enc_hid与dec_hid不相等,则应先用全连接层将语义编码enc_gru_final_state张量形状变换为(2, batch_size, dec_hid)，
        # 之后语义编码张量enc_gru_final_state才能作为解码器类中深度GRU循环网络的两个隐藏层的初始隐藏状态h_0; 若此时enc_hid与dec_hid相等,
        # 则语义编码张量enc_gru_final_state可直接用作解码器类中深度GRU循环网络的两个隐藏层的初始隐藏状态h_0
        if self.dec_hid != self.enc_hid:
            hidden_dim_transform = nn.Linear(self.enc_hid, self.dec_hid)
            h_0 = hidden_dim_transform( enc_gru_final_state )
        else:
            h_0 = enc_gru_final_state

        # (1, batch_size, enc_hid) --> (seq_len, batch_size, enc_hid)
        weighted_enc_gru_outputs = weighted_enc_gru_outputs.repeat(input.shape[0], 1, 1)
        # 此时注意力层Seq2SeqAttention计算得出的编码器每一时间步(time step)的隐藏状态hidden_state的加权求和结果张量weighted_enc_gru_outputs
        # 的目前形状为:(seq_len, batch_size, enc_hid); 此时将输入解码器的翻译后法文batch样本数据张量input(目前形状为(seq_len, batch_size，embed_size))
        # 与weighted_enc_gru_outputs在第三个维度(dim=2)上拼接,拼接后的input张量形状为(seq_len, batch_size, embed_size + enc_hid)
        input = torch.cat((input, weighted_enc_gru_outputs), dim=2)

        # 此时dec_gru_outputs张量的形状为: (seq_len, batch_size, dec_hid)。
        # 此时dec_gru_final_state张量的形状为: (num_layers * num_directions, batch_size, dec_hid) --> (2, batch_size, enc_hid),
        dec_gru_outputs, dec_gru_final_state = self.dec_gru(input, h_0)

        # 此时dec_gru_outputs张量的形状变为：(seq_len, batch_size, trg_vocab_size)
        dec_gru_outputs = self.fc_decoder_to_predictions(dec_gru_outputs)

        return dec_gru_outputs


# 利用Seq2Seq()将编码器类Seq2SeqEncoder()、注意力机制类Seq2SeqAttention()以及解码器类Seq2SeqDecoder()合并到一个模型中,并利用
# 待翻译英文batch样本数据和翻译后法文batch样本数据对模型进行训练与测试(预测)
class Seq2Seq(nn.Module):
    def __init__(self,Seq2SeqEncoder: nn.Module,
                 Seq2SeqAttention: nn.Module,
                 Seq2SeqDecoder: nn.Module,
                 src_vocab_size:int,
                 trg_vocab_size:int,
                 embed_size:int,
                 enc_hid:int,
                 dec_hid:int,
                 enc_num_layers = 2,
                 dec_num_layers = 2,
                 atten_dim = 8,
                 dropout = 0):

        # 实例化编码器类Seq2SeqEncoder()
        self.seq2seqEncoder = Seq2SeqEncoder(src_vocab_size, embed_size, enc_hid, enc_num_layers, dropout)
        # 实例化注意力机制类Seq2SeqAttention()
        self.seq2seqAttention = Seq2SeqAttention(enc_hid, dec_hid, atten_dim)
        # 实例化解码器类Seq2SeqDecoder()
        self.seq2seqDecoder = Seq2SeqDecoder(trg_vocab_size, embed_size, dec_hid, enc_hid, enc_num_layers, dropout)

    def forward(self,src_batch, trg_batch):
        # src_batch为当前输入模型的待翻译英文batch样本数据, trg_batch为当前输入模型的翻译后法文batch样本数据

        # 此时enc_gru_outputs张量的形状为: (seq_len, batch_size, enc_hid)。
        # 此时enc_gru_final_state张量的形状为: (num_layers * num_directions, batch_size, enc_hid) --> (2, batch_size, enc_hid),
        # 为最后一个时间步两个隐藏层的隐藏状态。
        enc_gru_outputs, enc_gru_final_state = self.seq2seqEncoder(src_batch)

        # 此时weighted_enc_gru_outputs张量形状为(1, batch_size, enc_hid)
        weighted_enc_gru_outputs = self.seq2seqAttention(enc_gru_outputs, enc_gru_final_state)

        # 此时dec_gru_outputs张量形状为(seq_len, batch_size, trg_vocab_size)
        dec_gru_outputs = self.seq2seqDecoder(trg_batch, enc_gru_final_state, weighted_enc_gru_outputs)

        return dec_gru_outputs






if __name__ == "__main__":
    a = ["我", "你", "撇", "了", "我", "你", "撇"]
    b = ["ABC", "a"]
    # vocab = Vocab(a)
    print(a[0:-1])




    # X = torch.randn((32,10,10))
    # val_len = torch.randint(low=1,high=10,size=(32,)) # 一维张量
    #
    # a = torch.arange(X.shape[1])[None,:]  # 此时a为一个1行10列的向量: 形状(1,10)
    # mask = a < val_len[:, None] # 此时val_len[:, None]为一个32行1列的向量: 形状(batch_size,1) —> (32,1)
    #
    # print("a[None,:]:",a)
    # print("val_len[:, None]:", val_len[:, None])
    # print("\nmask:",mask)
    #
    # X[~mask] = 0
    # print("\nX:",X)


    # a = torch.tensor([[1,2,3],[6,7,8]])
    # index = torch.tensor([[2],[1]])
    # b = a.gather(dim=1,index=index)
    # print(b)


    # Y = torch.zeros((3,10))
    # index = torch.tensor([1,0,0]).unsqueeze(1)
    # Y.scatter_(dim=1,index=index,value=-1)
    # print("Y: ",Y)
    #
    # labels = torch.tensor([1,0,0])
    # # loss = torch.nn.CrossEntropyLoss(ignore_index=0,reduction="sum")
    # loss = torch.nn.NLLLoss(ignore_index=0, reduction="sum")
    # loss_val = loss(Y,labels)
    # print("\nloss_val: ",loss_val.item())
